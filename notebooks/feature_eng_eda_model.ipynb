{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_df.pkl', 'rb') as f:\n",
    "    audio_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc_mean1</th>\n",
       "      <th>mfcc_mean2</th>\n",
       "      <th>mfcc_mean3</th>\n",
       "      <th>mfcc_mean4</th>\n",
       "      <th>mfcc_mean5</th>\n",
       "      <th>mfcc_mean6</th>\n",
       "      <th>mfcc_mean7</th>\n",
       "      <th>mfcc_mean8</th>\n",
       "      <th>mfcc_mean9</th>\n",
       "      <th>mfcc_mean10</th>\n",
       "      <th>...</th>\n",
       "      <th>chroma_std7</th>\n",
       "      <th>chroma_std8</th>\n",
       "      <th>chroma_std9</th>\n",
       "      <th>chroma_std10</th>\n",
       "      <th>chroma_std11</th>\n",
       "      <th>chroma_std12</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>rolloff_std</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-131.953600</td>\n",
       "      <td>100.331800</td>\n",
       "      <td>-136.209613</td>\n",
       "      <td>59.544095</td>\n",
       "      <td>-41.002218</td>\n",
       "      <td>18.666155</td>\n",
       "      <td>-29.881801</td>\n",
       "      <td>7.720259</td>\n",
       "      <td>-13.521617</td>\n",
       "      <td>-2.472400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082744</td>\n",
       "      <td>0.086488</td>\n",
       "      <td>0.103079</td>\n",
       "      <td>0.091772</td>\n",
       "      <td>0.083112</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>4190.474661</td>\n",
       "      <td>789.164506</td>\n",
       "      <td>6</td>\n",
       "      <td>fold1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-212.984794</td>\n",
       "      <td>113.986069</td>\n",
       "      <td>-27.193258</td>\n",
       "      <td>22.607542</td>\n",
       "      <td>0.675223</td>\n",
       "      <td>4.756774</td>\n",
       "      <td>15.624546</td>\n",
       "      <td>15.003598</td>\n",
       "      <td>12.439341</td>\n",
       "      <td>11.358548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235415</td>\n",
       "      <td>0.252663</td>\n",
       "      <td>0.193080</td>\n",
       "      <td>0.210450</td>\n",
       "      <td>0.206592</td>\n",
       "      <td>0.201269</td>\n",
       "      <td>3779.575026</td>\n",
       "      <td>698.880879</td>\n",
       "      <td>2</td>\n",
       "      <td>fold1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-58.958201</td>\n",
       "      <td>87.640273</td>\n",
       "      <td>-70.350945</td>\n",
       "      <td>-2.818272</td>\n",
       "      <td>-22.161617</td>\n",
       "      <td>-7.152818</td>\n",
       "      <td>-19.660093</td>\n",
       "      <td>-5.150989</td>\n",
       "      <td>-23.104785</td>\n",
       "      <td>-5.124065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122456</td>\n",
       "      <td>0.133950</td>\n",
       "      <td>0.129118</td>\n",
       "      <td>0.114089</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>0.122561</td>\n",
       "      <td>4129.396225</td>\n",
       "      <td>773.733583</td>\n",
       "      <td>4</td>\n",
       "      <td>fold1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-365.765009</td>\n",
       "      <td>61.840576</td>\n",
       "      <td>8.505344</td>\n",
       "      <td>33.080384</td>\n",
       "      <td>6.873962</td>\n",
       "      <td>-5.327185</td>\n",
       "      <td>-3.849906</td>\n",
       "      <td>-5.238709</td>\n",
       "      <td>-4.314379</td>\n",
       "      <td>-4.959619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244525</td>\n",
       "      <td>0.233069</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.231544</td>\n",
       "      <td>0.270749</td>\n",
       "      <td>0.263057</td>\n",
       "      <td>5021.219348</td>\n",
       "      <td>1082.168871</td>\n",
       "      <td>2</td>\n",
       "      <td>fold1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-84.870940</td>\n",
       "      <td>116.204518</td>\n",
       "      <td>-24.585843</td>\n",
       "      <td>9.107674</td>\n",
       "      <td>6.315533</td>\n",
       "      <td>3.426022</td>\n",
       "      <td>4.600642</td>\n",
       "      <td>4.316528</td>\n",
       "      <td>-0.700944</td>\n",
       "      <td>11.300272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112480</td>\n",
       "      <td>0.111136</td>\n",
       "      <td>0.099302</td>\n",
       "      <td>0.101390</td>\n",
       "      <td>0.095279</td>\n",
       "      <td>0.106619</td>\n",
       "      <td>3822.828136</td>\n",
       "      <td>411.186238</td>\n",
       "      <td>7</td>\n",
       "      <td>fold1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mfcc_mean1  mfcc_mean2  mfcc_mean3  mfcc_mean4  mfcc_mean5  mfcc_mean6  \\\n",
       "0 -131.953600  100.331800 -136.209613   59.544095  -41.002218   18.666155   \n",
       "1 -212.984794  113.986069  -27.193258   22.607542    0.675223    4.756774   \n",
       "2  -58.958201   87.640273  -70.350945   -2.818272  -22.161617   -7.152818   \n",
       "3 -365.765009   61.840576    8.505344   33.080384    6.873962   -5.327185   \n",
       "4  -84.870940  116.204518  -24.585843    9.107674    6.315533    3.426022   \n",
       "\n",
       "   mfcc_mean7  mfcc_mean8  mfcc_mean9  mfcc_mean10  ...    chroma_std7  \\\n",
       "0  -29.881801    7.720259  -13.521617    -2.472400  ...       0.082744   \n",
       "1   15.624546   15.003598   12.439341    11.358548  ...       0.235415   \n",
       "2  -19.660093   -5.150989  -23.104785    -5.124065  ...       0.122456   \n",
       "3   -3.849906   -5.238709   -4.314379    -4.959619  ...       0.244525   \n",
       "4    4.600642    4.316528   -0.700944    11.300272  ...       0.112480   \n",
       "\n",
       "   chroma_std8  chroma_std9  chroma_std10  chroma_std11  chroma_std12  \\\n",
       "0     0.086488     0.103079      0.091772      0.083112      0.087800   \n",
       "1     0.252663     0.193080      0.210450      0.206592      0.201269   \n",
       "2     0.133950     0.129118      0.114089      0.124617      0.122561   \n",
       "3     0.233069     0.224638      0.231544      0.270749      0.263057   \n",
       "4     0.111136     0.099302      0.101390      0.095279      0.106619   \n",
       "\n",
       "   rolloff_mean  rolloff_std  label   fold  \n",
       "0   4190.474661   789.164506      6  fold1  \n",
       "1   3779.575026   698.880879      2  fold1  \n",
       "2   4129.396225   773.733583      4  fold1  \n",
       "3   5021.219348  1082.168871      2  fold1  \n",
       "4   3822.828136   411.186238      7  fold1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    1000\n",
       "5    1000\n",
       "4    1000\n",
       "3    1000\n",
       "2    1000\n",
       "9    1000\n",
       "0    1000\n",
       "8     929\n",
       "1     429\n",
       "6     374\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold4     990\n",
       "fold5     936\n",
       "fold3     925\n",
       "fold2     888\n",
       "fold1     873\n",
       "fold7     838\n",
       "fold10    837\n",
       "fold6     823\n",
       "fold9     816\n",
       "fold8     806\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df.fold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = audio_df.loc[:, 'fold']\n",
    "y = audio_df.loc[:, 'label']\n",
    "X = audio_df.iloc[:, 0:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Note on Train/Test**\n",
    "The author's of this data set mention that they extracted the initial audio clips from different longer recordings. Thus, it's possible that if you naively shuffled, you could have a clip in the train and a different clip in the test that came from the same original recording. To combat this, they created 10 different folds corresponding to the 10 different original long recordings that were cut into 8,732 pieces. They suggest running 10-fold CV as the main (averaged) metric using the pre-determined folds, which will combat the problem above.\n",
    "\n",
    "HOWEVER - I'd still like to pursue a modified standard train/validate(in CV loop)/test approach. To do this I propose the following:\n",
    "1. Create 10 different splits: 1 fold for holdout, 9 folds for training/validation\n",
    "2. Train 9-fold CV on the remaining data resulting from holding out 1\n",
    "3. Report average test score. This way, the scored-test data was truly never seen in hyperparameter tuning phase.\n",
    "\n",
    "Note I did run some initial models using their proposed 10-fold schema without doing this, so I have that here to see if any of this ends up making much of a difference:\n",
    "\n",
    "Results for non-augmented data set from initial pull (i.e. no pitch-shifting) with StandardScaler on 10-fold CV of pre-determined splits:\n",
    "1. Logistic Mean accuracy: 0.643 +- 0.036\n",
    "2. Gaussian NB Mean accuracy: 0.490 +- 0.033\n",
    "3. SVM (RBF) Mean accuracy: 0.713 +- 0.022\n",
    "\n",
    "Poorer performance: MinMax scaler, other types of SVM kernels, PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te1 = y.iloc[0:873]\n",
    "y_tr1 = y.iloc[873:]\n",
    "X_te1 = X.iloc[0:873, :]\n",
    "X_tr1 = X.iloc[873:, :]\n",
    "y_te1, y_tr1, X_te1, X_tr1 = np.array(y_te1), np.array(y_tr1), np.array(X_te1), np.array(X_tr1)\n",
    "groups_2 = groups.drop(groups.index[0:873])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te2 = y.iloc[873:1761]\n",
    "y_tr2 = pd.concat([y.iloc[0:873], y.iloc[1761:]])\n",
    "X_te2 = X.iloc[873:1761, :]\n",
    "X_tr2 = X.drop(X.index[873:1761])\n",
    "y_te2, y_tr2, X_te2, X_tr2 = np.array(y_te2), np.array(y_tr2), np.array(X_te2), np.array(X_tr2)\n",
    "groups_2 = groups.drop(groups.index[873:1761])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te3 = y.iloc[1761:2686]\n",
    "y_tr3 = pd.concat([y.iloc[0:1761], y.iloc[2686:]])\n",
    "X_te3 = X.iloc[1761:2686, :]\n",
    "X_tr3 = X.drop(X.index[1761:2686])\n",
    "y_te3, y_tr3, X_te3, X_tr3 = np.array(y_te3), np.array(y_tr3), np.array(X_te3), np.array(X_tr3)\n",
    "groups_3 = groups.drop(groups.index[1761:2686])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te4 = y.iloc[3676:4612]\n",
    "y_tr4 = pd.concat([y.iloc[0:3676], y.iloc[4612:]])\n",
    "X_te4 = X.iloc[3676:4612, :]\n",
    "X_tr4 = X.drop(X.index[3676:4612])\n",
    "y_te4, y_tr4, X_te4, X_tr4 = np.array(y_te4), np.array(y_tr4), np.array(X_te4), np.array(X_tr4)\n",
    "groups_4 = groups.drop(groups.index[3676:4612])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te5 = y.iloc[4612:5434]\n",
    "y_tr5 = pd.concat([y.iloc[0:4612], y.iloc[5434:]])\n",
    "X_te5 = X.iloc[4612:5434, :]\n",
    "X_tr5 = X.drop(X.index[4612:5434])\n",
    "y_te5, y_tr5, X_te5, X_tr5 = np.array(y_te5), np.array(y_tr5), np.array(X_te5), np.array(X_tr5)\n",
    "groups_5 = groups.drop(groups.index[4612:5434])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te6 = y.iloc[5434:6273]\n",
    "y_tr6 = pd.concat([y.iloc[0:5434], y.iloc[6273:]])\n",
    "X_te6 = X.iloc[5434:6273, :]\n",
    "X_tr6 = X.drop(X.index[5434:6273])\n",
    "y_te6, y_tr6, X_te6, X_tr6 = np.array(y_te6), np.array(y_tr6), np.array(X_te6), np.array(X_tr6)\n",
    "groups_6 = groups.drop(groups.index[5434:6273])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te7 = y.iloc[6273:7079]\n",
    "y_tr7 = pd.concat([y.iloc[0:6273], y.iloc[7079:]])\n",
    "X_te7 = X.iloc[6273:7079, :]\n",
    "X_tr7 = X.drop(X.index[6273:7079])\n",
    "y_te7, y_tr7, X_te7, X_tr7 = np.array(y_te7), np.array(y_tr7), np.array(X_te7), np.array(X_tr7)\n",
    "groups_7 = groups.drop(groups.index[6273:7079])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te8 = y.iloc[7079:7895]\n",
    "y_tr8 = pd.concat([y.iloc[0:7079], y.iloc[7895:]])\n",
    "X_te8 = X.iloc[7079:7895, :]\n",
    "X_tr8 = X.drop(X.index[7079:7895])\n",
    "y_te8, y_tr8, X_te8, X_tr8 = np.array(y_te8), np.array(y_tr8), np.array(X_te8), np.array(X_tr8)\n",
    "groups_8 = groups.drop(groups.index[7079:7895])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te9 = y.iloc[7895:8732]\n",
    "y_tr9 = pd.concat([y.iloc[0:7895], y.iloc[8732:]])\n",
    "X_te9 = X.iloc[7895:8732, :]\n",
    "X_tr9 = X.drop(X.index[7895:8732])\n",
    "y_te9, y_tr9, X_te9, X_tr9 = np.array(y_te9), np.array(y_tr9), np.array(X_te9), np.array(X_tr9)\n",
    "groups_9 = groups.drop(groups.index[7895:8732])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [groups_1, groups_2, groups_3, groups_4, groups_5,\n",
    "             groups_6, groups_7, groups_8, groups_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_list = [X_tr1, X_tr2, X_tr3, X_tr4, X_tr5, X_tr6, X_tr7, X_tr8, X_tr9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_list = [y_tr1, y_tr2, y_tr3, y_tr4, y_tr5, y_tr6, y_tr7, y_tr8, y_tr9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_list = [X_te1, X_te2, X_te3, X_te4, X_te5, X_te6, X_te7, X_te8, X_te9]\n",
    "y_te_list = [y_te1, y_te2, y_te3, y_te4, y_te5, y_te6, y_te7, y_te8, y_te9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Validation CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Logistic Train accuracy: 0.642 +- 0.002\n",
      "SVM Train accuracy: 0.708 +- 0.002\n",
      "Random Forest Train accuracy: 0.688 +- 0.001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "logr_mean_acc, gnb_mean_acc, svm_mean_acc, rfc_mean_acc = [], [], [], []\n",
    "logr_cv_mean, gnb_cv_mean, svm_cv_mean, rfc_cv_mean = [], [], [], []\n",
    "logr_cv_std, gnb_cv_std, svm_cv_std, rfc_cv_std = [], [], [], []\n",
    "\n",
    "for i in range(0,9):\n",
    "    print(i)\n",
    "    gkf = GroupKFold(n_splits=9)\n",
    "    for train, val in gkf.split(X_tr_list[i], y_tr_list[i], groups=group_list[i]):\n",
    "        X_tr, y_tr = X_tr_list[i][train], y_tr_list[i][train]\n",
    "        X_val, y_val = X_tr_list[i][val], y_tr_list[i][val]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_tr_scale = scaler.fit_transform(X_tr)\n",
    "        X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "        logr = LogisticRegression(C=1000)\n",
    "        logr.fit(X_tr_scale, y_tr)\n",
    "        logr_mean_acc.append(logr.score(X_val_scale, y_val))\n",
    "\n",
    "        svm_rbf = SVC(kernel=\"rbf\")\n",
    "        svm_rbf.fit(X_tr_scale, y_tr)\n",
    "        svm_mean_acc.append(svm_rbf.score(X_val_scale, y_val))\n",
    "\n",
    "        rfc = RandomForestClassifier(n_estimators=128)\n",
    "        rfc.fit(X_tr_scale, y_tr)\n",
    "        rfc_mean_acc.append(rfc.score(X_val_scale, y_val))\n",
    "    \n",
    "    logr_cv_mean.append(np.mean(logr_mean_acc))\n",
    "    logr_cv_std.append(np.std(logr_mean_acc))\n",
    "    svm_cv_mean.append(np.mean(svm_mean_acc))\n",
    "    svm_cv_std.append(np.std(svm_mean_acc))\n",
    "    rfc_cv_mean.append(np.mean(rfc_mean_acc))\n",
    "    rfc_cv_std.append(np.std(rfc_mean_acc))\n",
    "    \n",
    "print(f'Logistic Train accuracy: {np.mean(logr_cv_mean):.3f} +- {np.std(logr_cv_std):.3f}')\n",
    "print(f'SVM Train accuracy: {np.mean(svm_cv_mean):.3f} +- {np.std(svm_cv_std):.3f}')\n",
    "print(f'Random Forest Train accuracy: {np.mean(rfc_cv_mean):.3f} +- {np.std(rfc_cv_std):.3f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Test accuracy: 0.643 +- 0.037\n",
      "GaussianNB Test accuracy: 0.497 +- 0.026\n",
      "SVM Test accuracy: 0.713 +- 0.023\n",
      "Random Forest Test accuracy: 0.696 +- 0.042\n"
     ]
    }
   ],
   "source": [
    "logr_test_acc, gnb_test_acc, svm_test_acc, rfc_test_acc = [], [], [], []\n",
    "\n",
    "for i in range(0,9):\n",
    "    X_tr, y_tr = X_tr_list[i], y_tr_list[i]\n",
    "    X_te, y_te = X_te_list[i], y_te_list[i]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scale = scaler.fit_transform(X_tr)\n",
    "    X_te_scale = scaler.transform(X_te)\n",
    "    \n",
    "    logr = LogisticRegression(C=1000)\n",
    "    logr.fit(X_tr_scale, y_tr)\n",
    "    logr_test_acc.append(logr.score(X_te_scale, y_te))\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_tr_scale, y_tr)\n",
    "    gnb_test_acc.append(gnb.score(X_te_scale, y_te))\n",
    "\n",
    "    svm_rbf = SVC(kernel=\"rbf\")\n",
    "    svm_rbf.fit(X_tr_scale, y_tr)\n",
    "    svm_test_acc.append(svm_rbf.score(X_te_scale, y_te))\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators=128)\n",
    "    rfc.fit(X_tr_scale, y_tr)\n",
    "    rfc_test_acc.append(rfc.score(X_te_scale, y_te))\n",
    "    \n",
    "print(f'Logistic Test accuracy: {np.mean(logr_test_acc):.3f} +- {np.std(logr_test_acc):.3f}')\n",
    "print(f'GaussianNB Test accuracy: {np.mean(gnb_test_acc):.3f} +- {np.std(gnb_test_acc):.3f}')\n",
    "print(f'SVM Test accuracy: {np.mean(svm_test_acc):.3f} +- {np.std(svm_test_acc):.3f}')\n",
    "print(f'Random Forest Test accuracy: {np.mean(rfc_test_acc):.3f} +- {np.std(rfc_test_acc):.3f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Mean accuracy: 0.643 +- 0.036\n",
      "Gaussian NB Mean accuracy: 0.490 +- 0.033\n",
      "SVM Mean accuracy: 0.713 +- 0.022\n",
      "Random Forest Mean accuracy: 0.685 +- 0.039\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# gkf = GroupKFold(n_splits=10)\n",
    "# logr_mean_acc, gnb_mean_acc, svm_mean_acc, rfc_mean_acc = [], [], [], []\n",
    "\n",
    "# for train, test in gkf.split(X, y, groups=groups):\n",
    "#     X_tr, y_tr = X[train], y[train]\n",
    "#     X_te, y_te = X[test], y[test]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_tr_scale = scaler.fit_transform(X_tr)\n",
    "#     X_te_scale = scaler.transform(X_te)\n",
    "    \n",
    "#     logr = LogisticRegression(C=1000)\n",
    "#     logr.fit(X_tr_scale, y_tr)\n",
    "#     logr_mean_acc.append(logr.score(X_te_scale, y_te))\n",
    "    \n",
    "#     gnb = GaussianNB()\n",
    "#     gnb.fit(X_tr_scale, y_tr)\n",
    "#     gnb_mean_acc.append(gnb.score(X_te_scale, y_te))\n",
    "    \n",
    "#     svm_rbf = SVC(kernel=\"rbf\")\n",
    "#     svm_rbf.fit(X_tr_scale, y_tr)\n",
    "#     svm_mean_acc.append(svm_rbf.score(X_te_scale, y_te))\n",
    "    \n",
    "#     rfc = RandomForestClassifier(n_estimators=128)\n",
    "#     rfc.fit(X_tr_scale, y_tr)\n",
    "#     rfc_mean_acc.append(rfc.score(X_te_scale, y_te))\n",
    "\n",
    "    \n",
    "# print(f'Logistic Mean accuracy: {np.mean(logr_mean_acc):.3f} +- {np.std(logr_mean_acc):.3f}')\n",
    "# print(f'Gaussian NB Mean accuracy: {np.mean(gnb_mean_acc):.3f} +- {np.std(gnb_mean_acc):.3f}')\n",
    "# print(f'SVM Mean accuracy: {np.mean(svm_mean_acc):.3f} +- {np.std(svm_mean_acc):.3f}')\n",
    "# print(f'Random Forest Mean accuracy: {np.mean(rfc_mean_acc):.3f} +- {np.std(rfc_mean_acc):.3f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: \n",
    "1. Present accuracy of different models in terms of boxplots from above\n",
    "2. Get this in terms of F1, since treats all class labels indifferently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
