{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_audio.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O8n_iIwDJ39x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import torch.utils.data as data_utils\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnRWZyGcziUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f3c38b2e-47be-4ac3-dbc4-2dead1d0226f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E0sRHJQoX_-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/features_50.pkl', 'rb') as f:\n",
        "  features = pickle.load(f)\n",
        "  \n",
        "with open('/content/gdrive/My Drive/folds_50.pkl', 'rb') as f:\n",
        "  folds = pickle.load(f)\n",
        "  \n",
        "with open('/content/gdrive/My Drive/labels_50.pkl', 'rb') as f:\n",
        "  labels = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vuQTq6sqoLT0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numpy to Tensors"
      ]
    },
    {
      "metadata": {
        "id": "VOZK2FTHM4Xc",
        "colab_type": "code",
        "outputId": "51f6bf86-d4d9-47c1-ee0c-e3b6682a6056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(features), len(folds), len(labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3676 3676 3676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "htY0rw53NEeB",
        "colab_type": "code",
        "outputId": "0f27569f-90f7-49d6-f392-9e90d56c605a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "features[0].shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "Az-ul-6DPMdT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features_arr = np.asarray(features, dtype=np.float32)\n",
        "labels_arr = np.asarray(labels, dtype=np.int64)\n",
        "folds_array = np.asarray(folds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiXijxbpbY6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features_arr = np.swapaxes(features_arr, 1, 3)\n",
        "features_arr = np.swapaxes(features_arr, 2, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9A7w4yFgs5ZY",
        "colab_type": "code",
        "outputId": "1dfe068c-127e-423e-f0c7-c3d63c91b488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(features_arr.shape, labels_arr.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3676, 3, 50, 50) (3676,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yx0FBeYmYUjo",
        "colab_type": "code",
        "outputId": "648ec2a5-548d-4da8-df30-31847fab92c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "unique_items, counts = np.unique(folds, return_counts=True)\n",
        "unique_items, counts"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['fold1', 'fold2', 'fold3', 'fold4'], dtype='<U5'),\n",
              " array([873, 888, 925, 990]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "9PzLnU19c3UI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sum1 = counts[0]\n",
        "sum12 = counts[0] + counts[1]\n",
        "sum23 = sum12 + counts[2]\n",
        "sum34 = sum23 + counts[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dmoxTe8nerPn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_te1 = labels_arr[0:sum1]\n",
        "y_tr1 = labels_arr[sum1:]\n",
        "X_te1 = features_arr[0:sum1, :, :, :]\n",
        "X_tr1 = features_arr[sum1:, :, :, :]\n",
        "folds_1 = np.delete(folds_array, list(range(0, sum1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fu24jCsqgLtS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_te2 = labels_arr[sum1:sum12]\n",
        "y_tr2 = np.concatenate([labels_arr[0:sum1], labels_arr[sum12:]])\n",
        "X_te2 = features_arr[sum1:sum12, :, :, :]\n",
        "X_tr2 = np.delete(features_arr, slice(sum1, sum12), axis=0)\n",
        "folds_2 = np.delete(folds_array, list(range(sum1, sum12)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77E6MRkPkbBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_te3 = labels_arr[sum12:sum23]\n",
        "y_tr3 = np.concatenate([labels_arr[0:sum12], labels_arr[sum23:]])\n",
        "X_te3 = features_arr[sum12:sum23, :, :, :]\n",
        "X_tr3 = np.delete(features_arr, slice(sum12,sum23), axis=0)\n",
        "folds_3 = np.delete(folds_array, list(range(sum12,sum23)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_vysU0qkyK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_te4 = labels_arr[sum23:sum34]\n",
        "y_tr4 = np.concatenate([labels_arr[0:sum23], labels_arr[sum34:]])\n",
        "X_te4 = features_arr[sum23:sum34, :, :, :]\n",
        "X_tr4 = np.delete(features_arr, slice(sum23,sum34), axis=0)\n",
        "folds_4 = np.delete(folds_array, list(range(sum23,sum34)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dNyU0qRlOYp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "group_list = [folds_1, folds_2, folds_3, folds_4]\n",
        "X_train_list = [X_tr1, X_tr2, X_tr3, X_tr4]\n",
        "y_train_list = [y_tr1, y_tr2, y_tr3, y_tr4]\n",
        "X_test_list = [X_te1, X_te2, X_te3, X_te4]\n",
        "y_test_list = [y_te1, y_te2, y_te3, y_te4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4yxmiNJoRCl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ]
    },
    {
      "metadata": {
        "id": "uuHgSgE7k3vA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_transformations = transforms.Compose([\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_transformations = transforms.Compose([\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "\n",
        "])\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).float()\n",
        "        self.target = torch.from_numpy(target).long()\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "train_loaders = []\n",
        "test_loaders = []\n",
        "for i in range(4):\n",
        "  \n",
        "  numpy_data_tr = X_train_list[i]\n",
        "  numpy_target_tr = y_train_list[i]\n",
        "  \n",
        "  \n",
        "  dataset_tr = MyDataset(numpy_data_tr, numpy_target_tr, transform=train_transformations)\n",
        "  loader_tr = DataLoader(\n",
        "      dataset_tr,\n",
        "      batch_size=32,\n",
        "      shuffle=False,\n",
        "      num_workers=2,\n",
        "      pin_memory=torch.cuda.is_available()\n",
        "  )\n",
        "  \n",
        "  numpy_data_te = X_test_list[i]\n",
        "  numpy_target_te = y_test_list[i]\n",
        "  \n",
        "  dataset_te = MyDataset(numpy_data_te, numpy_target_te, transform=test_transformations)\n",
        "  loader_te = DataLoader(\n",
        "      dataset_te,\n",
        "      batch_size=32,\n",
        "      shuffle=False,\n",
        "      num_workers=2,\n",
        "      pin_memory=torch.cuda.is_available()\n",
        "  )\n",
        "  \n",
        "  train_loaders.append(loader_tr)\n",
        "  test_loaders.append(loader_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DgfiByvwNxdk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YK2TcSdJmO0g",
        "colab_type": "code",
        "outputId": "1383d36b-1419-4957-a731-6cfaa7c6562b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1935
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
        "        self.dropout1 = nn.Dropout(.03)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv22 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv23 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv24 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=12)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
        "        self.dropout2 = nn.Dropout(.03)\n",
        "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=36, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=36, out_channels=36, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv42 = nn.Conv2d(in_channels=36, out_channels=36, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv43 = nn.Conv2d(in_channels=36, out_channels=36, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv44 = nn.Conv2d(in_channels=36, out_channels=36, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=36)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
        "        self.dropout3 = nn.Dropout(.03)\n",
        "        self.conv5 = nn.Conv2d(in_channels=36, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv61 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv62 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv63 = nn.Conv2d(in_channels=48, out_channels=48, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=48)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=4)           \n",
        "        self.fc = nn.Linear(in_features=432, out_features=num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "#         print(input.shape)\n",
        "\n",
        "        output = self.conv1(input)\n",
        "        output = self.bn1(output)\n",
        "\n",
        "        output = self.relu1(output)\n",
        "    \n",
        "#         print(output.shape)\n",
        "\n",
        "        output = self.pool1(output)\n",
        "        output = self.dropout1(output)\n",
        "\n",
        "#         print(output.shape)\n",
        "\n",
        "#         print(output.shape)\n",
        "#         print(output.shape)\n",
        "        \n",
        "        output = self.conv2(output)\n",
        "        output = self.conv22(output)\n",
        "        output = self.conv23(output)\n",
        "        output = self.conv24(output)\n",
        "        output = self.bn2(output)\n",
        "\n",
        "        output = self.relu2(output)\n",
        "\n",
        "#         print(output.shape)\n",
        "#         print(output.shape)\n",
        "#         print(output.shape)\n",
        "\n",
        "        output = self.pool2(output)\n",
        "        output = self.dropout2(output)\n",
        "\n",
        "#         print(output.shape)\n",
        "\n",
        "\n",
        "        output = self.conv3(output)\n",
        "        output = self.conv4(output)\n",
        "        output = self.conv42(output)\n",
        "        output = self.conv43(output)\n",
        "        output = self.conv44(output)\n",
        "        output = self.bn3(output)\n",
        "\n",
        "        output = self.relu3(output)\n",
        "\n",
        "#         print(output.shape)\n",
        "#         print(output.shape)\n",
        "\n",
        "#         print(output.shape)\n",
        "        output = self.pool3(output)\n",
        "        output = self.dropout3(output)\n",
        "  #         output = self.pool3(output)\n",
        "\n",
        "        output = self.conv5(output)\n",
        "        output = self.conv6(output)\n",
        "        output = self.conv61(output)\n",
        "        output = self.conv62(output)\n",
        "        output = self.conv63(output)\n",
        "        output = self.bn4(output)\n",
        "        output = self.relu4(output)\n",
        "\n",
        "        output = self.pool4(output)\n",
        "#         output = self.dropout4(output)\n",
        "\n",
        "        outout = self.avgpool(output)\n",
        "\n",
        "#         print(output.shape)\n",
        "        output = output.view(-1, 432)\n",
        "#         print(output.shape)\n",
        "        output = self.fc(output)\n",
        "#         print(output.shape)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "log_freq = 40\n",
        "\n",
        "cuda_avail = torch.cuda.is_available()\n",
        "\n",
        "model = SimpleNet()\n",
        "\n",
        "if cuda_avail:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=0.00008)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "def save_models(epoch):\n",
        "    torch.save(model.state_dict(), \"conv_{}.model\".format(epoch))\n",
        "    print(\"Checkpoint saved\")\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_acc = 0.0\n",
        "    running_loss = 0.0\n",
        "    running_f1 = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loaders[1]):\n",
        "      \n",
        "        if cuda_avail:\n",
        "                images = Variable(images.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "\n",
        "        output = model(images)\n",
        "\n",
        "        loss = loss_fn(output, labels)\n",
        "        running_loss += loss.item()\n",
        "        y_pred = output.argmax(dim=-1)  \n",
        "        \n",
        "        running_f1 += f1_score(labels.cpu(), y_pred.cpu(), average='macro')\n",
        "        \n",
        "    average_loss = running_loss / len(test_loaders[1])\n",
        "    average_f1 = running_f1 / (len(test_loaders[1]))\n",
        "    \n",
        "    \n",
        "    print(f'Val Loss: {average_loss:.5f} Val f1: {average_f1:.5f}\\n')\n",
        "    test_acc = test_acc / len(test_loaders[1])\n",
        "    return average_f1\n",
        "\n",
        "  \n",
        "  \n",
        "def train(num_epochs):\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_f1 = 0.0\n",
        "        running_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loaders[1]):\n",
        "            \n",
        "            if cuda_avail:\n",
        "                images = Variable(images.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "\n",
        "            output = model(images)\n",
        "            loss = loss_fn(output,labels)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "           \n",
        "            running_loss += loss.item()\n",
        "            y_pred = output.argmax(dim=-1)\n",
        "            running_f1 += f1_score(labels.cpu(), y_pred.cpu(), average='macro')\n",
        "            if i % log_freq == log_freq - 1:\n",
        "              average_loss = running_loss / len(train_loaders[1])\n",
        "              average_f1 = running_f1 / len(train_loaders[1])\n",
        "              print(f'Mini-batch: {i + 1}/{len(train_loaders[1])} '\n",
        "                  f'Loss: {average_loss:.5f} f1: {average_f1:.5f}')\n",
        "              running_loss = 0.0\n",
        "              running_f1 = 0.0\n",
        "\n",
        "        average_f1 = test()\n",
        "        \n",
        "        if average_f1 > best_f1:\n",
        "            save_models(epoch)\n",
        "            best_f1 = average_f1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(25)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mini-batch: 40/88 Loss: 0.98119 f1: 0.06156\n",
            "Mini-batch: 80/88 Loss: 0.92519 f1: 0.06199\n",
            "Val Loss: 2.06181 Val f1: 0.09851\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.87391 f1: 0.10455\n",
            "Mini-batch: 80/88 Loss: 0.84405 f1: 0.11486\n",
            "Val Loss: 1.94572 Val f1: 0.20077\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.80134 f1: 0.14909\n",
            "Mini-batch: 80/88 Loss: 0.78804 f1: 0.14328\n",
            "Val Loss: 1.88711 Val f1: 0.22956\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.74897 f1: 0.17344\n",
            "Mini-batch: 80/88 Loss: 0.75200 f1: 0.14890\n",
            "Val Loss: 1.84751 Val f1: 0.24192\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.70633 f1: 0.18654\n",
            "Mini-batch: 80/88 Loss: 0.71760 f1: 0.16471\n",
            "Val Loss: 1.78455 Val f1: 0.27529\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.67387 f1: 0.20003\n",
            "Mini-batch: 80/88 Loss: 0.68460 f1: 0.17800\n",
            "Val Loss: 1.77506 Val f1: 0.28900\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.64169 f1: 0.21224\n",
            "Mini-batch: 80/88 Loss: 0.65872 f1: 0.19243\n",
            "Val Loss: 1.74905 Val f1: 0.31728\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.61442 f1: 0.21438\n",
            "Mini-batch: 80/88 Loss: 0.63303 f1: 0.20439\n",
            "Val Loss: 1.74139 Val f1: 0.31375\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.59210 f1: 0.22369\n",
            "Mini-batch: 80/88 Loss: 0.60950 f1: 0.21289\n",
            "Val Loss: 1.72983 Val f1: 0.33131\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.56861 f1: 0.23041\n",
            "Mini-batch: 80/88 Loss: 0.59025 f1: 0.21976\n",
            "Val Loss: 1.71682 Val f1: 0.35352\n",
            "\n",
            "Checkpoint saved\n",
            "Mini-batch: 40/88 Loss: 0.54705 f1: 0.23912\n",
            "Mini-batch: 80/88 Loss: 0.56889 f1: 0.23511\n",
            "Val Loss: 1.74297 Val f1: 0.34172\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.52740 f1: 0.25507\n",
            "Mini-batch: 80/88 Loss: 0.54818 f1: 0.23813\n",
            "Val Loss: 1.77413 Val f1: 0.31486\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.50927 f1: 0.25712\n",
            "Mini-batch: 80/88 Loss: 0.53200 f1: 0.24934\n",
            "Val Loss: 1.76285 Val f1: 0.32028\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.49469 f1: 0.26443\n",
            "Mini-batch: 80/88 Loss: 0.51651 f1: 0.24805\n",
            "Val Loss: 1.76441 Val f1: 0.33252\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.47617 f1: 0.27604\n",
            "Mini-batch: 80/88 Loss: 0.49513 f1: 0.25756\n",
            "Val Loss: 1.76797 Val f1: 0.34274\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.45076 f1: 0.28417\n",
            "Mini-batch: 80/88 Loss: 0.48061 f1: 0.26711\n",
            "Val Loss: 1.80068 Val f1: 0.32200\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.44988 f1: 0.27956\n",
            "Mini-batch: 80/88 Loss: 0.46664 f1: 0.27243\n",
            "Val Loss: 1.83187 Val f1: 0.32516\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.42808 f1: 0.29129\n",
            "Mini-batch: 80/88 Loss: 0.45527 f1: 0.26819\n",
            "Val Loss: 1.78025 Val f1: 0.32904\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.41821 f1: 0.29586\n",
            "Mini-batch: 80/88 Loss: 0.44277 f1: 0.28357\n",
            "Val Loss: 1.86105 Val f1: 0.31928\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.40261 f1: 0.30420\n",
            "Mini-batch: 80/88 Loss: 0.42331 f1: 0.28726\n",
            "Val Loss: 1.84216 Val f1: 0.32491\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.38452 f1: 0.31149\n",
            "Mini-batch: 80/88 Loss: 0.41058 f1: 0.29436\n",
            "Val Loss: 1.85382 Val f1: 0.33056\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.37581 f1: 0.31582\n",
            "Mini-batch: 80/88 Loss: 0.39897 f1: 0.29395\n",
            "Val Loss: 1.79597 Val f1: 0.34588\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.36331 f1: 0.31974\n",
            "Mini-batch: 80/88 Loss: 0.38895 f1: 0.29850\n",
            "Val Loss: 1.85113 Val f1: 0.33353\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.34871 f1: 0.32408\n",
            "Mini-batch: 80/88 Loss: 0.37891 f1: 0.30027\n",
            "Val Loss: 1.86658 Val f1: 0.33836\n",
            "\n",
            "Mini-batch: 40/88 Loss: 0.33539 f1: 0.33388\n",
            "Mini-batch: 80/88 Loss: 0.36519 f1: 0.31117\n",
            "Val Loss: 1.85530 Val f1: 0.34516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T8VdQ-dT07tH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}